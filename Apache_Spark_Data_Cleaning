
# Title     : Apache Spark - AI Engineering - Based on the Lending Club data from Kaggle
# Objective : Apache Spark using DataBricks (pySpark)
# Created by: Abhishek Gaur
# Created on: 9/30/2020


#File location and type
file_location = "/FileStore/tables/accepted2018.csv"
file_type = "csv"

#CSV options
infer_schema = "true"
first_row_is_header = "true"
delimiter = ","

df= spark.read.format(file_type) \
    .option("inferSchema", infer_schema) \
    .option("header", first_row_is_header) \
    .option("sep", delimiter) \
    .load(file_location)

display(df)



temp_table_name = "loanstats"

df.createOrReplaceTempView(temp_table_name)

%sql


select * from loanstats


%sql
select count(*) from loanstats

df.describe().show()

#Reducing the number of columns to improve the readibility of the describe output
df_sel.describe("term","loan_amnt","emp_length", "annual_inc","dti","delinq_2yrs","revol_util","total_acc").show()


#removing the text like 'year', 'years', '>' and '+' from the emp_length column, to get a better mathematical description of the column
from pyspark.sql.functions import regexp_replace, regexp_extract
from pyspark.sql.functions import col

# Using the regexp_replace
regex_string='years|year|\\+|\\<'
df_sel.select(regexp_replace(col("emp_length"), regex_string, "").alias("emplength_cleaned"),col("emp_length")).show(10)

# Using the regexp_extract
regex_string="\\d+"
df_sel.select(regexp_extract(col("emp_length"), regex_string, 0).alias("emplength_cleaned"),col("emp_length")).show(10)

#adding a column with "Yes / no" to describe bad or good loan
df_sel=df_sel.withColumn("bad_loan", when(df_sel.loan_status.isin(["Late (31-120 days)", "Charged Off", "In Grace Period","Late (16-30 days)"]),'Yes').otherwise('No'))

#Storing the cleaned dataframe into a permanent table
permanent_table_name = "lc_loan_data"

df_sel.write.format("parquet").saveAsTable(permanent_table_name)
