# Title     : Classfication of IRIS flower
# Objective : Machine Learning code to implement the flower classification for IRIS flower into Setosa, VersiColor, Virginica
# Created by: Abhishek Gaur
# Created on: July-30-2020

import numpy as np


import matplotlib.pyplot as plt

from scipy import sparse
# initially faced some issues using sklearn, it was showing some incompatibility with the numpy. So upgraded numpy package, to resolve that error.

from sklearn import svm
from sklearn.datasets import load_iris


iris = load_iris()
print(iris.keys())

# loading the number of samples and features into vars and then displaying them along with the first row of the dataset
n_samples, n_features = iris.data.shape
print('Number of samples:', n_samples)
print('Number of features:', n_features)
#print(iris.data[0])

#print(iris.data.shape)
#print(iris.target.shape)
print(iris.target_names)
#print(iris.target)
#print(iris.data)


#to get the count across the various classes in the data, to check the distribution of the data
# to do this, we use the bincount function from numpy

print(np.bincount(iris.target))





# drawing a histogram on a particular feature
# index 0= sepal length, 1= sepal width, 2=petal length, 3 =petal width
x_index = 3
colors = ['blue', 'red', 'green']

#for label, color in zip(range(len(iris.target_names)), colors):
 #   plt.hist(iris.data[iris.target == label, x_index],
  #           label=iris.target_names[label],
   #          color=color)

#plt.xlabel(iris.feature_names[x_index])
#plt.legend(loc='upper right')
#plt.show()


# drawing a ScatterPlot on a particular feature
# index 0= sepal length, 1= sepal width, 2=petal length, 3 =petal width
x_index = 1
y_index= 2
colors = ['blue', 'red', 'green']

for label, color in zip(range(len(iris.target_names)), colors):
    plt.scatter(iris.data[iris.target == label, x_index],
                iris.data[iris.target == label, y_index],
             label=iris.target_names[label],
             c=color)

plt.xlabel(iris.feature_names[x_index])
plt.ylabel(iris.feature_names[y_index])

plt.legend(loc='upper left')
plt.show()







# Setting a random seed for reproducibility
rnd = np.random.RandomState(seed=123)

# generating a random array
x = rnd.uniform(low=0.0,high=1.0,size=(3,5))

# print(x)

# print the transpose of the matrix X
#print(x.T)

# Usage of Sparse Matrix
# generating a random array
x1 = rnd.uniform(low=0.0,high=1.0,size=(10,5))
#print(x1)
x1[x1 < 0.7] = 0
#print(x1)

xsparse = sparse.csr_matrix(x1)
#print(xsparse)

# print it back into a normal array
#print(xsparse.toarray())

# using matplotlib
#d = np.linspace (0,10,100)
#plt.plot(d, np.sin(d))
#plt.show()

#x = np.random.normal(size=500)
#y = np.random.normal(size=500)
#plt.scatter(x,y)
#plt.show()


from sklearn.datasets import get_data_home
print(get_data_home())


# Trying to work with the faces dataset
from sklearn.datasets import fetch_olivetti_faces

faces = fetch_olivetti_faces()
fig = plt.figure(figsize=(6,6))
fig.subplots_adjust(left=0,right=1,bottom=0,top=1,hspace=0.05,wspace=0.05)
for i in range(64):
    ax=fig.add_subplot(8,8,i+1,xticks=[],yticks=[])
    ax.imshow(faces.images[i],cmap=plt.cm.bone, interpolation='nearest')

#plt.show()


# using nearest neighbour to divide the dataset into training and test data sets
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier

#iris = load_iris() - commented out since its already loaded above
x, y = iris.data, iris.target

classifier = KNeighborsClassifier()
#print(x)

from sklearn.model_selection import train_test_split
# Reference code was using sklearn.cross_validation, but it has since been replaced by sklearn.model_selection

train_X, test_X, train_Y, test_Y = train_test_split(x, y, train_size=0.5, random_state=123,stratify=y)
print("Labels for training and testing data")
print(train_Y)
print(test_Y)

# tocheck how balanced the test and training sets are
print('All:',np.bincount(y)/float(len(y))*100)
print('All:',np.bincount(train_Y)/float(len(train_Y))*100)
print('All:',np.bincount(test_Y)/float(len(test_Y))*100)

# since it isnt very balanced, we introduce the stratify=y in the above train_test_split function

classifier.fit(train_X, train_Y)
pred_Y = classifier.predict(test_X)
print("Fraction Correct [Accuracy]:")
print(np.sum(pred_Y==test_Y)/float(len(test_Y)))


# Visualizing the correct and incorrect predictions
#print("\nsamples correctly classified")
#correct_idx = np.where(pred_Y == test_Y)[0]
#print(correct_idx)

#print("\nsamples incorrectly classified")
#incorrect_idx = np.where(pred_Y != test_Y)[0]
#print(incorrect_idx)

